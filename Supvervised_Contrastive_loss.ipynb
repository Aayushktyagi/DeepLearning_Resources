{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUP_CNN_SCL",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnhvDYoF/yidSbNjAsvX8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayushktyagi/DeepLearning_Resources/blob/master/Supvervised_Contrastive_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k9trZSqBTa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49651d2-d2dd-4eac-f79b-6f60eb8f8054"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "LP-We5k76aIr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArHXymjbBmzu"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time \n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparamter\n",
        "ohe = False\n",
        "projection_units = 128\n",
        "input_shape = (13250,)\n",
        "temperature = 0.5\n",
        "learning_rate = 0.0001\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "8F0L_Vrq5CJ0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfL9kWWwBntN"
      },
      "source": [
        "dataset_path = '/content/drive/MyDrive/ColabNotebooks/Cancer/CUP/TCGA_FPKM_5_log.csv'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEwT7NScCiTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7461cf-2f77-4a5f-fb95-e548e4349cae"
      },
      "source": [
        "# Data pre-processing\n",
        "# Load dataset\n",
        "raw_dataframe = pd.read_csv(dataset_path, sep=',')\n",
        "dataframe = raw_dataframe.copy()\n",
        "print(dataframe.shape)\n",
        "print(dataframe.head())\n",
        "cat_label = dataframe.pop('label')\n",
        "# drop gene id\n",
        "dataframe = dataframe.drop(['Unnamed: 0'],axis = 1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14237, 13252)\n",
            "                             Unnamed: 0       A2M  ...    ZNF749  label\n",
            "0  3609e1bc-c1ed-4569-8b06-5c057e360ab6  1.827073  ...  0.291297   MESO\n",
            "1  68a26147-3686-4e65-8eb4-9179b5bc0e57  1.602108  ...  0.274832   MESO\n",
            "2  5da3d163-55d7-4310-bccb-e26e26848318  1.988105  ...  0.250790   MESO\n",
            "3  da1c29c8-a1df-4f08-b685-36c4f308298b  1.639228  ...  0.288733   MESO\n",
            "4  1a3a9d02-12ea-4ab9-9cb8-e333ad51bfb5  2.217213  ...  0.262579   MESO\n",
            "\n",
            "[5 rows x 13252 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMp5zRxXCjjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7b949a-db5a-4446-ea64-8a480fe5032f"
      },
      "source": [
        "# Dictionary for key value pair of labels \n",
        "num_classes = np.unique(cat_label)\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(cat_label)\n",
        "# print(list(le.classes_))\n",
        "label = le.transform(cat_label)\n",
        "print(\"label:{}\".format(label))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:[19 19 19 ...  6  6  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESFtOyYLCnzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa6f15d-ac5f-400c-d44a-9f60b2853069"
      },
      "source": [
        "classes_list = list(le.classes_)\n",
        "print(classes_list)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ACC', 'ALL', 'AML', 'BLCA', 'BRAIN', 'BRCA', 'CESC', 'CHOL', 'CML', 'CORE', 'ESCA', 'HNSC', 'KICH', 'KIRC', 'KIRP', 'KIWT', 'LIHC', 'LUAD', 'LUSC', 'MESO', 'MRT', 'NBL', 'NHL', 'OS', 'OV', 'PAAD', 'PCPG', 'PCT', 'PRAD', 'SARC', 'SKCM', 'STAD', 'TGCT', 'THCA', 'THYM', 'UCEC', 'UVM']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEQ6uEj9CsxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3991557f-f947-43ce-8662-f58f7ceeab17"
      },
      "source": [
        "inv_label = list(le.inverse_transform(label))\n",
        "used = set()\n",
        "ordered_class_list = [x for x in inv_label if x not in used and (used.add(x) or True)]\n",
        "print(ordered_class_list)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MESO', 'BRCA', 'PAAD', 'UVM', 'AML', 'PCT', 'KIRP', 'LIHC', 'OV', 'CML', 'UCEC', 'TGCT', 'MRT', 'NBL', 'KIRC', 'ACC', 'ALL', 'OS', 'PCPG', 'ESCA', 'LUAD', 'KICH', 'HNSC', 'CORE', 'NHL', 'BRAIN', 'KIWT', 'CHOL', 'PRAD', 'BLCA', 'SARC', 'THYM', 'LUSC', 'THCA', 'STAD', 'SKCM', 'CESC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM4cmzyACtm5"
      },
      "source": [
        "def normal_dataframe(dataframe , norm_type  , normalize = False):\n",
        "    if normalize == True:\n",
        "        if norm_type =='min_max':\n",
        "            min_max_scaler = preprocessing.MinMaxScaler()\n",
        "            dataframe_scaled = min_max_scaler.fit_transform(dataframe)\n",
        "            return dataframe_scaled\n",
        "        \n",
        "        elif norm_type == 'Stand_scaler':\n",
        "            scaler = preprocessing.StandardScaler()\n",
        "            dataframe_scaled = scaler.fit_transform(dataframe)\n",
        "            return dataframe_scaled\n",
        "        \n",
        "        elif norm_type == 'zscore':\n",
        "#             data_stats = dataframe.describe()\n",
        "#             dataframe_scaled = (dataframe - data_stats['mean'])/ dataframe['std']\n",
        "            dataframe_scaled = stats.zscore(dataframe)\n",
        "            return dataframe_scaled\n",
        "        \n",
        "        elif norm_type == 'MaxAbsScaler':\n",
        "            max_abs_scaler = preprocessing.MaxAbsScaler()\n",
        "            dataframe_scaled = max_abs_scaler.fit_transform(dataframe)\n",
        "            return dataframe_scaled\n",
        "            \n",
        "        elif norm_type == 'log_magnitude':\n",
        "            dataframe_scaled = dataframe.apply(lambda x : np.log(x+1)) \n",
        "            return dataframe_scaled.to_numpy()\n",
        "        \n",
        "        elif norm_type == 'log_normalize':\n",
        "            dataframe_scaled = dataframe.apply(lambda x : np.log(x+1))\n",
        "            nlise = preprocessing.Normalizer()\n",
        "            dataframe_normalized = nlise.fit_transform(dataframe_scaled)\n",
        "            return dataframe_normalized\n",
        "    else:\n",
        "        return dataframe.to_numpy()\n",
        "    \n",
        "# dataframe_scaled = normal_dataframe(dataframe ,'Stand_scaler', True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5sFQRChpreI"
      },
      "source": [
        "dataframe_scaled = dataframe"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKFiXa-BDKyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650f740b-01ef-4fb2-89d5-9e49e289b28f"
      },
      "source": [
        "#Test train Split\n",
        "# dataframe = dataframe.drop(['gene_id'],axis = 1)\n",
        "train, test , Y_train , Y_test = train_test_split(dataframe_scaled, label ,test_size = 0.2)\n",
        "train , val , Y_train , Y_val = train_test_split(train ,Y_train , test_size = 0.2)\n",
        "print(\"trainig data:{}\".format(len(train)))\n",
        "print(\"Validation data:{}\".format(len(val)))\n",
        "print(\"Testing data:{}\".format(len(test)))\n",
        "# print(\"Data Sample:{}\".format(train[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainig data:9111\n",
            "Validation data:2278\n",
            "Testing data:2848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-TtO5mdtYw8"
      },
      "source": [
        "def label_encoder(label):\n",
        "  # encode class values as integers\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(label)\n",
        "  encoded_Y = encoder.transform(label)\n",
        "  # convert integers to dummy variables (i.e. one hot encoded)\n",
        "  dummy_y = np_utils.to_categorical(encoded_Y)\n",
        "  return dummy_y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIbwO6PItZiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e04537-8cfd-4514-e447-0ce0b36edf63"
      },
      "source": [
        "# creating label one hot encoding\n",
        "if ohe:\n",
        "    train_label_oh = label_encoder(Y_train)\n",
        "    test_label_oh = label_encoder(Y_test)\n",
        "    val_label_oh = label_encoder(Y_val)\n",
        "else:\n",
        "    train_label_oh = Y_train\n",
        "    test_label_oh = Y_test\n",
        "    val_label_oh = Y_val\n",
        "\n",
        "print(\"Data Shape : Train:{} , Val :{} ,Test:{}\".format(np.shape(train) ,np.shape(val), np.shape(test)))\n",
        "print(\"Label Shape :Train:{} , Val :{} ,Test:{}\".format(np.shape(train_label_oh),np.shape(val_label_oh), np.shape(test_label_oh)))\n",
        "# print(train.keys())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape : Train:(9111, 13250) , Val :(2278, 13250) ,Test:(2848, 13250)\n",
            "Label Shape :Train:(9111,) , Val :(2278,) ,Test:(2848,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZE9o4oCEpvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd023144-803b-4ade-fcb3-f557c0aa6cf6"
      },
      "source": [
        "# Build encoder model\n",
        "def build_cup_encoder():\n",
        "    encoder_in = tf.keras.Input(input_shape)\n",
        "    layer1 = Dense(10000 , activation = 'relu' , input_shape= (13250,) , name = 'Input')(encoder_in)\n",
        "    layer2 = Dense(5000, activation ='relu' , name = \"Hidden_Layer_2\")(layer1)\n",
        "    layer3 = Dense(1000, activation = 'relu' , name = 'Hidden_layer_3')(layer2)\n",
        "    layer4 = Dense(500, activation = 'relu', name = 'Hidden_layer_4')(layer3)\n",
        "    model = Model(inputs = encoder_in, outputs = layer4, name = 'encoder_model')\n",
        "    return model\n",
        "\n",
        "encoder = build_cup_encoder()\n",
        "encoder.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 13250)]           0         \n",
            "                                                                 \n",
            " Input (Dense)               (None, 10000)             132510000 \n",
            "                                                                 \n",
            " Hidden_Layer_2 (Dense)      (None, 5000)              50005000  \n",
            "                                                                 \n",
            " Hidden_layer_3 (Dense)      (None, 1000)              5001000   \n",
            "                                                                 \n",
            " Hidden_layer_4 (Dense)      (None, 500)               500500    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 188,016,500\n",
            "Trainable params: 188,016,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(encoder, trainable = True):\n",
        "\n",
        "    for layers in encoder.layers:\n",
        "      layers.trainable = trainable\n",
        "    classifier_input = tf.keras.Input(input_shape)\n",
        "    encoder_out = build_cup_encoder()(classifier_input)\n",
        "    layer5 = Dense(100 , activation ='relu' , name = 'Hidden_layer_5')(encoder_out)\n",
        "    layer6 = Dense(37 , activation = 'softmax' , name = \"Output_layer\")(layer5)\n",
        "    \n",
        "    model = Model(inputs = classifier_input, outputs = layer6, name = 'classifier_model')\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    \n",
        "    model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "               optimizer = optimizer,\n",
        "               metrics = [keras.metrics.SparseCategoricalAccuracy()])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = build_classifier(encoder)"
      ],
      "metadata": {
        "id": "xXrnqkcM5M6q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Self supervised Loss\n",
        "class SupervisedContrastiveLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n"
      ],
      "metadata": {
        "id": "vCPeJxb-5NpV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build Projection Head\n",
        "def build_projection_head(encoder):\n",
        "    projection_input = tf.keras.Input(input_shape)\n",
        "    encoder_output = encoder(projection_input)\n",
        "    \n",
        "    projection_layer = Dense(projection_units, activation=\"relu\")(encoder_output)\n",
        "    model = Model(inputs = projection_input, outputs = projection_layer)\n",
        "    return model\n",
        "\n",
        "projection_head = build_projection_head(encoder)"
      ],
      "metadata": {
        "id": "ywgmmeK65cTs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = build_cup_encoder()\n",
        "encoder_with_projection_head = build_projection_head(encoder)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature),\n",
        ")\n",
        "\n",
        "encoder_with_projection_head.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwsutidk5dUT",
        "outputId": "3e8fda5b-850b-4392-92cb-d90da0998737"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 13250)]           0         \n",
            "                                                                 \n",
            " encoder_model (Functional)  (None, 500)               188016500 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               64128     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 188,080,628\n",
            "Trainable params: 188,080,628\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SCL\n",
        "history = encoder_with_projection_head.fit(train,train_label_oh,\n",
        "           validation_data=(val,val_label_oh),\n",
        "           epochs = 500,\n",
        "           batch_size = 64,\n",
        "           shuffle = True,\n",
        "           verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9vtGdDh5izV",
        "outputId": "628a4365-6065-450d-b7a6-049073044090"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "143/143 [==============================] - 16s 96ms/step - loss: 3.3645 - val_loss: 3.1970\n",
            "Epoch 2/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 3.0891 - val_loss: 2.9454\n",
            "Epoch 3/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 3.0559 - val_loss: 2.9556\n",
            "Epoch 4/100\n",
            "143/143 [==============================] - 13s 92ms/step - loss: 2.9357 - val_loss: 2.8517\n",
            "Epoch 5/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.9073 - val_loss: 3.0422\n",
            "Epoch 6/100\n",
            "143/143 [==============================] - 14s 95ms/step - loss: 2.8627 - val_loss: 2.8983\n",
            "Epoch 7/100\n",
            "143/143 [==============================] - 14s 95ms/step - loss: 2.7783 - val_loss: 2.7640\n",
            "Epoch 8/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.8047 - val_loss: 2.8786\n",
            "Epoch 9/100\n",
            "143/143 [==============================] - 14s 95ms/step - loss: 2.8965 - val_loss: 3.2574\n",
            "Epoch 10/100\n",
            "143/143 [==============================] - 14s 95ms/step - loss: 2.9336 - val_loss: 2.7525\n",
            "Epoch 11/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.7880 - val_loss: 2.8656\n",
            "Epoch 12/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.7784 - val_loss: 2.7191\n",
            "Epoch 13/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.7056 - val_loss: 2.6737\n",
            "Epoch 14/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.7047 - val_loss: 2.7201\n",
            "Epoch 15/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.7021 - val_loss: 2.6525\n",
            "Epoch 16/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.7076 - val_loss: 2.7423\n",
            "Epoch 17/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6763 - val_loss: 2.7899\n",
            "Epoch 18/100\n",
            "143/143 [==============================] - 14s 94ms/step - loss: 2.6979 - val_loss: 2.7155\n",
            "Epoch 19/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.7003 - val_loss: 2.6401\n",
            "Epoch 20/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6562 - val_loss: 2.6153\n",
            "Epoch 21/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.6166 - val_loss: 2.6119\n",
            "Epoch 22/100\n",
            "143/143 [==============================] - 14s 95ms/step - loss: 2.6240 - val_loss: 2.6253\n",
            "Epoch 23/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6179 - val_loss: 2.6179\n",
            "Epoch 24/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.6550 - val_loss: 2.6821\n",
            "Epoch 25/100\n",
            "143/143 [==============================] - 14s 95ms/step - loss: 2.7051 - val_loss: 2.6817\n",
            "Epoch 26/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6910 - val_loss: 2.7726\n",
            "Epoch 27/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.7610 - val_loss: 2.7397\n",
            "Epoch 28/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.7432 - val_loss: 2.9059\n",
            "Epoch 29/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.7877 - val_loss: 2.6781\n",
            "Epoch 30/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6848 - val_loss: 2.6801\n",
            "Epoch 31/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.7215 - val_loss: 2.7079\n",
            "Epoch 32/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6715 - val_loss: 2.6502\n",
            "Epoch 33/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.6513 - val_loss: 2.6407\n",
            "Epoch 34/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6548 - val_loss: 2.6161\n",
            "Epoch 35/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5992 - val_loss: 2.6229\n",
            "Epoch 36/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.6072 - val_loss: 2.6223\n",
            "Epoch 37/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5949 - val_loss: 2.5995\n",
            "Epoch 38/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.6030 - val_loss: 2.5909\n",
            "Epoch 39/100\n",
            "143/143 [==============================] - 13s 93ms/step - loss: 2.5956 - val_loss: 2.6182\n",
            "Epoch 40/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5992 - val_loss: 2.5897\n",
            "Epoch 41/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.6068 - val_loss: 2.6220\n",
            "Epoch 42/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6035 - val_loss: 2.6227\n",
            "Epoch 43/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6575 - val_loss: 2.6400\n",
            "Epoch 44/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6007 - val_loss: 2.5956\n",
            "Epoch 45/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5816 - val_loss: 2.6218\n",
            "Epoch 46/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5803 - val_loss: 2.6155\n",
            "Epoch 47/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6223 - val_loss: 2.5862\n",
            "Epoch 48/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.5778 - val_loss: 2.5756\n",
            "Epoch 49/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5994 - val_loss: 2.5883\n",
            "Epoch 50/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.5803 - val_loss: 2.5899\n",
            "Epoch 51/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6148 - val_loss: 2.5817\n",
            "Epoch 52/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5722 - val_loss: 2.6030\n",
            "Epoch 53/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5769 - val_loss: 2.5739\n",
            "Epoch 54/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5770 - val_loss: 2.5716\n",
            "Epoch 55/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5680 - val_loss: 2.6018\n",
            "Epoch 56/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.7029 - val_loss: 2.6353\n",
            "Epoch 57/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.6432 - val_loss: 2.7628\n",
            "Epoch 58/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5997 - val_loss: 2.5862\n",
            "Epoch 59/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5872 - val_loss: 2.6009\n",
            "Epoch 60/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5915 - val_loss: 2.5958\n",
            "Epoch 61/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5879 - val_loss: 2.5970\n",
            "Epoch 62/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6099 - val_loss: 2.6246\n",
            "Epoch 63/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.6052 - val_loss: 2.6002\n",
            "Epoch 64/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5903 - val_loss: 2.6002\n",
            "Epoch 65/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5642 - val_loss: 2.5874\n",
            "Epoch 66/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5552 - val_loss: 2.5799\n",
            "Epoch 67/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5515 - val_loss: 2.5754\n",
            "Epoch 68/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.5742 - val_loss: 2.5795\n",
            "Epoch 69/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5787 - val_loss: 2.5708\n",
            "Epoch 70/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5669 - val_loss: 2.6011\n",
            "Epoch 71/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5789 - val_loss: 2.5921\n",
            "Epoch 72/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5664 - val_loss: 2.5720\n",
            "Epoch 73/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5626 - val_loss: 2.5831\n",
            "Epoch 74/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5594 - val_loss: 2.5911\n",
            "Epoch 75/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5493 - val_loss: 2.5691\n",
            "Epoch 76/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5505 - val_loss: 2.5630\n",
            "Epoch 77/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5501 - val_loss: 2.5652\n",
            "Epoch 78/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5683 - val_loss: 2.5695\n",
            "Epoch 79/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5653 - val_loss: 2.5843\n",
            "Epoch 80/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5582 - val_loss: 2.5629\n",
            "Epoch 81/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5652 - val_loss: 2.5745\n",
            "Epoch 82/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5735 - val_loss: 2.6165\n",
            "Epoch 83/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.6109 - val_loss: 2.6183\n",
            "Epoch 84/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6815 - val_loss: 2.7098\n",
            "Epoch 85/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.6945 - val_loss: 2.7073\n",
            "Epoch 86/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6292 - val_loss: 2.6108\n",
            "Epoch 87/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5933 - val_loss: 2.6158\n",
            "Epoch 88/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5884 - val_loss: 2.6115\n",
            "Epoch 89/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.5854 - val_loss: 2.6145\n",
            "Epoch 90/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.5916 - val_loss: 2.6470\n",
            "Epoch 91/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6755 - val_loss: 2.6367\n",
            "Epoch 92/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.6081 - val_loss: 2.5784\n",
            "Epoch 93/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5696 - val_loss: 2.5821\n",
            "Epoch 94/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.5994 - val_loss: 2.5874\n",
            "Epoch 95/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.5832 - val_loss: 2.6554\n",
            "Epoch 96/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6326 - val_loss: 2.7846\n",
            "Epoch 97/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.7440 - val_loss: 2.7040\n",
            "Epoch 98/100\n",
            "143/143 [==============================] - 13s 90ms/step - loss: 2.7195 - val_loss: 2.6880\n",
            "Epoch 99/100\n",
            "143/143 [==============================] - 13s 94ms/step - loss: 2.6598 - val_loss: 2.6548\n",
            "Epoch 100/100\n",
            "143/143 [==============================] - 13s 91ms/step - loss: 2.6617 - val_loss: 2.6480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = build_classifier(encoder, trainable = False)"
      ],
      "metadata": {
        "id": "zuBdRBDp5l7g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(patience=25, verbose=1,monitor='val_loss'),\n",
        "    ReduceLROnPlateau(factor=0.05, patience=15, min_lr=0.000000001, verbose=1,monitor='val_loss'),\n",
        "    # ModelCheckpoint('/home/aayush.t/Ashick/CUP/Model/CUP_DNN_defaultNonorm_tcga.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "]\n"
      ],
      "metadata": {
        "id": "9CbicO4jed2m"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model training \n",
        "history = classifier.fit(\n",
        "           train,train_label_oh,\n",
        "           validation_data=(val,val_label_oh),\n",
        "           epochs = 100,\n",
        "           batch_size = 32,\n",
        "           shuffle = True,\n",
        "           callbacks=callbacks,\n",
        "           verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MHw632j5oID",
        "outputId": "6bae8378-3522-4fad-c0c3-013b7356773d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "285/285 [==============================] - 25s 85ms/step - loss: 1.1315 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5063 - val_sparse_categorical_accuracy: 0.8507 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.3204 - sparse_categorical_accuracy: 0.9063 - val_loss: 0.2170 - val_sparse_categorical_accuracy: 0.9425 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.2412 - sparse_categorical_accuracy: 0.9323 - val_loss: 0.2624 - val_sparse_categorical_accuracy: 0.9241 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.2185 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.2175 - val_sparse_categorical_accuracy: 0.9399 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.1820 - sparse_categorical_accuracy: 0.9470 - val_loss: 0.1705 - val_sparse_categorical_accuracy: 0.9500 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.1687 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.2005 - val_sparse_categorical_accuracy: 0.9421 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.1451 - sparse_categorical_accuracy: 0.9553 - val_loss: 0.1686 - val_sparse_categorical_accuracy: 0.9486 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.1401 - sparse_categorical_accuracy: 0.9574 - val_loss: 0.1678 - val_sparse_categorical_accuracy: 0.9530 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.1226 - sparse_categorical_accuracy: 0.9627 - val_loss: 0.2073 - val_sparse_categorical_accuracy: 0.9434 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.1016 - sparse_categorical_accuracy: 0.9671 - val_loss: 0.1520 - val_sparse_categorical_accuracy: 0.9579 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.1122 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.1502 - val_sparse_categorical_accuracy: 0.9579 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.1008 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.1726 - val_sparse_categorical_accuracy: 0.9522 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9697 - val_loss: 0.1569 - val_sparse_categorical_accuracy: 0.9526 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0991 - sparse_categorical_accuracy: 0.9697 - val_loss: 0.1252 - val_sparse_categorical_accuracy: 0.9680 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9759 - val_loss: 0.1480 - val_sparse_categorical_accuracy: 0.9557 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0937 - sparse_categorical_accuracy: 0.9692 - val_loss: 0.1716 - val_sparse_categorical_accuracy: 0.9574 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0838 - sparse_categorical_accuracy: 0.9728 - val_loss: 0.1275 - val_sparse_categorical_accuracy: 0.9649 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9761 - val_loss: 0.1513 - val_sparse_categorical_accuracy: 0.9601 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0672 - sparse_categorical_accuracy: 0.9770 - val_loss: 0.1224 - val_sparse_categorical_accuracy: 0.9622 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0753 - sparse_categorical_accuracy: 0.9749 - val_loss: 0.2543 - val_sparse_categorical_accuracy: 0.9350 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0616 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.1305 - val_sparse_categorical_accuracy: 0.9640 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.1452 - val_sparse_categorical_accuracy: 0.9609 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "285/285 [==============================] - 24s 82ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.1625 - val_sparse_categorical_accuracy: 0.9583 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0549 - sparse_categorical_accuracy: 0.9838 - val_loss: 0.1456 - val_sparse_categorical_accuracy: 0.9579 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0484 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.1500 - val_sparse_categorical_accuracy: 0.9548 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0478 - sparse_categorical_accuracy: 0.9842 - val_loss: 0.1767 - val_sparse_categorical_accuracy: 0.9535 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0526 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.1410 - val_sparse_categorical_accuracy: 0.9653 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0523 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.1498 - val_sparse_categorical_accuracy: 0.9649 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0537 - sparse_categorical_accuracy: 0.9830 - val_loss: 0.1130 - val_sparse_categorical_accuracy: 0.9684 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "285/285 [==============================] - 24s 82ms/step - loss: 0.0534 - sparse_categorical_accuracy: 0.9828 - val_loss: 0.1335 - val_sparse_categorical_accuracy: 0.9675 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0513 - sparse_categorical_accuracy: 0.9829 - val_loss: 0.1168 - val_sparse_categorical_accuracy: 0.9680 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0427 - sparse_categorical_accuracy: 0.9853 - val_loss: 0.1340 - val_sparse_categorical_accuracy: 0.9596 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.1352 - val_sparse_categorical_accuracy: 0.9723 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0386 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.1192 - val_sparse_categorical_accuracy: 0.9688 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0351 - sparse_categorical_accuracy: 0.9885 - val_loss: 0.1547 - val_sparse_categorical_accuracy: 0.9671 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0461 - sparse_categorical_accuracy: 0.9842 - val_loss: 0.2579 - val_sparse_categorical_accuracy: 0.9372 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0582 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.1606 - val_sparse_categorical_accuracy: 0.9587 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0441 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.1290 - val_sparse_categorical_accuracy: 0.9684 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0327 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.1170 - val_sparse_categorical_accuracy: 0.9697 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9877 - val_loss: 0.1156 - val_sparse_categorical_accuracy: 0.9710 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.1442 - val_sparse_categorical_accuracy: 0.9631 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.1335 - val_sparse_categorical_accuracy: 0.9671 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.1983 - val_sparse_categorical_accuracy: 0.9557 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "285/285 [==============================] - ETA: 0s - loss: 0.0269 - sparse_categorical_accuracy: 0.9908\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.1647 - val_sparse_categorical_accuracy: 0.9675 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9977 - val_loss: 0.1098 - val_sparse_categorical_accuracy: 0.9723 - lr: 5.0000e-06\n",
            "Epoch 46/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.1104 - val_sparse_categorical_accuracy: 0.9728 - lr: 5.0000e-06\n",
            "Epoch 47/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.1107 - val_sparse_categorical_accuracy: 0.9723 - lr: 5.0000e-06\n",
            "Epoch 48/100\n",
            "285/285 [==============================] - 24s 82ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.1137 - val_sparse_categorical_accuracy: 0.9710 - lr: 5.0000e-06\n",
            "Epoch 49/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0042 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.1137 - val_sparse_categorical_accuracy: 0.9723 - lr: 5.0000e-06\n",
            "Epoch 50/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.1145 - val_sparse_categorical_accuracy: 0.9719 - lr: 5.0000e-06\n",
            "Epoch 51/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0037 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.1153 - val_sparse_categorical_accuracy: 0.9710 - lr: 5.0000e-06\n",
            "Epoch 52/100\n",
            "285/285 [==============================] - 23s 82ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.1168 - val_sparse_categorical_accuracy: 0.9732 - lr: 5.0000e-06\n",
            "Epoch 53/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1185 - val_sparse_categorical_accuracy: 0.9732 - lr: 5.0000e-06\n",
            "Epoch 54/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1202 - val_sparse_categorical_accuracy: 0.9710 - lr: 5.0000e-06\n",
            "Epoch 55/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0029 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.1198 - val_sparse_categorical_accuracy: 0.9728 - lr: 5.0000e-06\n",
            "Epoch 56/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.1233 - val_sparse_categorical_accuracy: 0.9728 - lr: 5.0000e-06\n",
            "Epoch 57/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1243 - val_sparse_categorical_accuracy: 0.9723 - lr: 5.0000e-06\n",
            "Epoch 58/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1285 - val_sparse_categorical_accuracy: 0.9719 - lr: 5.0000e-06\n",
            "Epoch 59/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1279 - val_sparse_categorical_accuracy: 0.9723 - lr: 5.0000e-06\n",
            "Epoch 60/100\n",
            "285/285 [==============================] - ETA: 0s - loss: 0.0024 - sparse_categorical_accuracy: 0.9992\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-07.\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.1280 - val_sparse_categorical_accuracy: 0.9719 - lr: 5.0000e-06\n",
            "Epoch 61/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.1278 - val_sparse_categorical_accuracy: 0.9719 - lr: 2.5000e-07\n",
            "Epoch 62/100\n",
            "285/285 [==============================] - 24s 84ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1276 - val_sparse_categorical_accuracy: 0.9723 - lr: 2.5000e-07\n",
            "Epoch 63/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.1275 - val_sparse_categorical_accuracy: 0.9723 - lr: 2.5000e-07\n",
            "Epoch 64/100\n",
            "285/285 [==============================] - 24s 83ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.1274 - val_sparse_categorical_accuracy: 0.9723 - lr: 2.5000e-07\n",
            "Epoch 65/100\n",
            "285/285 [==============================] - 24s 84ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.1274 - val_sparse_categorical_accuracy: 0.9723 - lr: 2.5000e-07\n",
            "Epoch 66/100\n",
            "191/285 [===================>..........] - ETA: 7s - loss: 0.0018 - sparse_categorical_accuracy: 0.9995"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnzpjdLtE_zB"
      },
      "source": [
        "#Results visualization\n",
        "def showloss(history):\n",
        "        # loss plot\n",
        "        plt.subplots(figsize=(15,10))\n",
        "        plt.subplot(2,1,1)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Model loss')\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('loss')\n",
        "        plt.legend(['Train','Validation'],loc = 'upper left')\n",
        "\n",
        "        #accuracy plot\n",
        "        plt.subplot(2,1,2)\n",
        "        plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "        plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "        plt.title('Accuracy')\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.legend(['Train','Validation'] , loc = 'upper left')\n",
        "        plt.show()\n",
        "\n",
        "showloss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test):\n",
        "    predictions = classifier.predict(test, verbose = 1)\n",
        "    predict_index = predictions.argmax(axis=1)\n",
        "    test_index = test_label_oh\n",
        "    results = confusion_matrix(test_index , predict_index)\n",
        "\n",
        "\n",
        "    #Classification accuracy\n",
        "    accuracy = accuracy_score(test_index,predict_index)\n",
        "    print(\"Classification accuracy:{}\".format(accuracy))\n",
        "    print()\n",
        "    print(accuracy)\n",
        "\n",
        "    #Classification report \n",
        "    classify_report = classification_report(test_index,predict_index)\n",
        "    print(\"Classification report\")\n",
        "    print(classify_report)\n",
        "    \n",
        "    return test_label_oh, predictions, test_index, predict_index\n",
        "    \n",
        "test_label_oh, predictions, test_index, predict_index = predict(test)"
      ],
      "metadata": {
        "id": "7O2YSl825w2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N_dUom1pKKQm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}