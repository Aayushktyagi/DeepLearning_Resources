{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.transform import rescale, resize\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "# KERAS IMPORTS\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.layers import concatenate, add\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Input\n",
    "# from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import Conv2D\n",
    "# from tensorflow.keras.layers import Conv2DTranspose\n",
    "# from tensorflow.keras.layers import MaxPool2D, AvgPool2D\n",
    "# from tensorflow.keras.layers import UpSampling2D\n",
    "# # from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
    "# from tensorflow.keras.layers import LeakyReLU\n",
    "# from tensorflow.keras.layers import Activation\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import Lambda\n",
    "# from tensorflow.keras.layers import MaxPooling2D, GlobalMaxPool2D\n",
    "# from tensorflow.keras.layers import Flatten\n",
    "# from tensorflow.keras.layers import Reshape\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.layers import Add, Multiply\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "\n",
    "# import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import concatenate, add\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import MaxPool2D, AvgPool2D\n",
    "from keras.layers import UpSampling2D\n",
    "# from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Add, Multiply\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "im_width = 128\n",
    "im_height = 128\n",
    "n_channels = 3\n",
    "border = 5\n",
    "n_filters=16\n",
    "dropout=0.05\n",
    "batchnorm=True\n",
    "b_size = 16\n",
    "path_train = './Dataset/Compaq_orignal/Compaq_orignal/Compaq_orignal/train/'\n",
    "path_valid = './Dataset/Compaq_orignal/Compaq_orignal/Compaq_orignal/test/'\n",
    "path_test = './Dataset/NIR_Dataset_New/'\n",
    "# path_test_NIR_pred = './predict_image/vae_unet_b4/'\n",
    "# path_train_comp_pred = './predict_image/vae_unet_compaq/train/'\n",
    "# path_test_comp_pred = './predict_image/vae_unet_compaq/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img_size = 128\n",
    "def get_data(train_data_path):\n",
    "    img_size = 128\n",
    "#     train_ids = next(os.walk(train_data_path))[1]\n",
    "    train_ids = next(os.walk(train_data_path + \"image/1\"))[2]\n",
    "    \n",
    "    x_train = np.zeros((len(train_ids), img_size, img_size, 3), dtype=np.uint8)\n",
    "    y_train = np.zeros((len(train_ids), img_size, img_size, 1), dtype=np.bool)\n",
    "\n",
    "    for i, id_ in tqdm_notebook(enumerate(train_ids), total=len(train_ids)):\n",
    "        path = train_data_path+\"image/1\"+\"/{}\".format(id_)\n",
    "        img = cv2.imread(path,1)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        x_train[i]=img\n",
    "\n",
    "        height, width, _ = img.shape\n",
    "        label = np.zeros((height, width, 1))\n",
    "        path2 = train_data_path+\"label/1/\"\n",
    "        mask_ = cv2.imread(path2+id_, 0)\n",
    "        mask_ = cv2.resize(mask_, (img_size, img_size))\n",
    "        mask_ = np.expand_dims(mask_, axis=-1)\n",
    "        label = np.maximum(label, mask_)\n",
    "        y_train[i]=label\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train).astype(np.uint8)\n",
    "    return x_train , y_train\n",
    "X_train, y_train = get_data(path_train)\n",
    "X_valid , y_valid = get_data(path_valid)\n",
    "X_test_NIR , y_test_NIR = get_data(path_test)\n",
    "# X_train_comp_pred , y_train_comp_pred = get_data(path_train_comp_pred)\n",
    "# X_valid_comp_pred , y_valid_comp_pred = get_data(path_test_comp_pred)\n",
    "# X_test_NIR_pred , y_test_NIR_pred = get_data(path_test_NIR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_test_NIR))\n",
    "X_test_NIR_test , X_test_NIR_comp , y_test_NIR , y_test_NIR_comp = train_test_split(X_test_NIR , y_test_NIR , test_size = 0.20 , random_state = 42)\n",
    "print(np.shape(X_test_NIR_test) , np.shape(X_test_NIR_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.zeros((3516, img_size, img_size, 3), dtype=np.uint8)\n",
    "y_final = np.zeros((3516, img_size, img_size, 1), dtype=np.bool)\n",
    "for i,image in enumerate(X_train):\n",
    "    X_final[i] = image\n",
    "    y_final[i] = y_train[i]\n",
    "for i, image in enumerate(X_test_NIR_comp):\n",
    "    X_final[i+3268]=image\n",
    "    y_final[i+3268]=y_test_NIR_comp[i]\n",
    "y_final = np.asarray(y_final).astype(np.uint8)\n",
    "X_final = np.asarray(X_final)\n",
    "print(np.shape(X_final) , np.shape(y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data looks all right\n",
    "ix = random.randint(0, len(X_final))\n",
    "# ix = 3515\n",
    "has_mask = y_final[ix].max() > 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# image = X_train[ix, ... , 0]\n",
    "image = X_final[ix,:,:,:].reshape(128,128,3)\n",
    "# image = (image + 1 ) / 2\n",
    "# image = image * 255\n",
    "ax[0].imshow(image.astype('uint8'))\n",
    "if has_mask:\n",
    "    ax[0].contour(y_final[ix].squeeze(), colors='k', levels=[0.5])\n",
    "ax[0].set_title('Image')\n",
    "\n",
    "ax[1].imshow(y_final[ix].squeeze(), interpolation='bilinear', cmap='gray')\n",
    "ax[1].set_title('Mask');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import efficientnet \n",
    "# K.clear_session()\n",
    "from efficientnet import EfficientNetB4\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation == True:\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16):\n",
    "    x = LeakyReLU(alpha=0.1)(blockInput)\n",
    "    x = BatchNormalization()(x)\n",
    "    blockInput = BatchNormalization()(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3) )\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    return x\n",
    "\n",
    "\n",
    "def UEfficientNet(input_shape=(None, None, 3),dropout_rate=0.1):\n",
    "\n",
    "    backbone = EfficientNetB4(weights='imagenet',\n",
    "                            include_top=False,\n",
    "                            input_shape=input_shape)\n",
    "    input = backbone.input\n",
    "    start_neurons = 16\n",
    "\n",
    "    conv4 = backbone.layers[342].output\n",
    "    conv4 = LeakyReLU(alpha=0.1)(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout_rate)(pool4)\n",
    "    \n",
    "     # Middle\n",
    "    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = LeakyReLU(alpha=0.1)(convm)\n",
    "    \n",
    "    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(dropout_rate)(uconv4)\n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "    uconv4 = LeakyReLU(alpha=0.1)(uconv4)\n",
    "    \n",
    "    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    conv3 = backbone.layers[154].output\n",
    "    uconv3 = concatenate([deconv3, conv3])    \n",
    "    uconv3 = Dropout(dropout_rate)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    conv2 = backbone.layers[92].output\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "        \n",
    "    uconv2 = Dropout(0.1)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n",
    "    \n",
    "    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    conv1 = backbone.layers[30].output\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    \n",
    "    uconv1 = Dropout(0.1)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n",
    "    \n",
    "    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n",
    "    uconv0 = Dropout(0.1)(uconv0)\n",
    "    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n",
    "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n",
    "    \n",
    "    uconv0 = Dropout(dropout_rate/2)(uconv0)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
    "    \n",
    "    model = Model(input, output_layer)\n",
    "    model.name = 'u-xception'\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "img_size = 128\n",
    "model = UEfficientNet(input_shape=(img_size,img_size,3),dropout_rate=0.20)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# def mean_iou(y_true, y_pred):\n",
    "#     y_pred = K.cast(K.greater(y_pred, .5), dtype='float32') # .5 is the threshold\n",
    "#     inter = K.sum(K.sum(K.squeeze(y_true * y_pred, axis=3), axis=2), axis=1)\n",
    "#     union = K.sum(K.sum(K.squeeze(y_true + y_pred, axis=3), axis=2), axis=1) - inter\n",
    "#     return K.mean((inter + K.epsilon()) / (union + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    y_pred = K.cast(K.greater(y_pred, .5), dtype='float32') # .5 is the threshold\n",
    "    inter = K.sum(K.sum(K.squeeze(y_true * y_pred, axis=3), axis=2), axis=1)\n",
    "    union = K.sum(K.sum(K.squeeze(y_true + y_pred, axis=3), axis=2), axis=1) - inter\n",
    "    return K.mean((inter + K.epsilon()) / (union + K.epsilon()))\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    '''Calculates the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    '''\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def dice_coe(y_true, y_pred, smooth = 100):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    '''Calculates the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    '''\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Calculates the mean accuracy rate across all predictions for binary\n",
    "    classification problems.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [2, 3]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(validation_data,\n",
    "                                          validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "            show_result = {}\n",
    "            for i, result in enumerate(results):\n",
    "                show_result[\"NIR_\" + self.model.metrics_names[i]] = results[i]\n",
    "                if i == 0:\n",
    "                    valuename = validation_set_name + '_loss'\n",
    "                else:\n",
    "                    valuename = validation_set_name + '_' + self.model.metrics_names[i-1]\n",
    "                self.history.setdefault(valuename, []).append(result)\n",
    "            print(\"           \")\n",
    "            print(\"NIR Results\")\n",
    "            print(show_result)\n",
    "            print(\"          \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test = get_data(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIR_history = AdditionalValidationSets([(X_test_NIR_test, y_test_NIR, 'NIR')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=bce_logdice_loss, optimizer=Adam(1e-4 , decay = 1e-6), metrics=[iou_metric,dice_coef])\n",
    "model.compile(loss=bce_logdice_loss, optimizer=keras.optimizers.RMSprop(lr=0.001 , decay=1e-3), metrics=[mean_iou,dice_coef,precision, recall, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1,monitor='val_loss'),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1,monitor='val_loss'),\n",
    "#     NIR_history,\n",
    "    ModelCheckpoint('./models/u-efficientnet/1/u-efficientnet_bce_meaniou_dice_oCompq_128_with_20p_NIR.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, batch_size=16, epochs=100, callbacks=callbacks,\n",
    "#                     validation_data=(X_valid, y_valid))\n",
    "\n",
    "history = model.fit(X_final, y_final, batch_size=16, epochs=100, callbacks=callbacks,\n",
    "                    validation_data=(X_test_NIR_test, y_test_NIR))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['iou_metric'][1:])\n",
    "plt.plot(history.history['val_iou_metric'][1:])\n",
    "plt.ylabel('iou')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "\n",
    "plt.title('model IOU')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'][1:])\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\")\n",
    "plt.ylabel('val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation','best_model'], loc='upper left')\n",
    "plt.title('model loss')\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model.load_weights('./models/u-efficientnet/1/u-efficientnet_bce_meaniou_dice_oCompq_128_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./models/u-efficientnet/1/u-efficientnet_20p_NIR_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate val\n",
    "model.evaluate(X_test_NIR_pred, y_test_NIR_pred, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "# Predict on train, val and test\n",
    "preds_train = model.predict(X_train, verbose=1)\n",
    "preds_val = model.predict(X_valid, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    has_mask = y[ix].max() > 0\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='gray')\n",
    "    if has_mask:\n",
    "        ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[0].set_title('gray')\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze())\n",
    "    ax[1].set_title('orignal')\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    if has_mask:\n",
    "        ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[2].set_title('Predicted')\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    if has_mask:\n",
    "        ax[3].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[3].set_title('Predicted binary');\n",
    "# Check if training data looks all right\n",
    "plot_sample(X_train, y_train, preds_train, preds_train_t, ix=14)\n",
    "\n",
    "# Check if valid data looks all right\n",
    "plot_sample(X_valid, y_valid, preds_val, preds_val_t, ix=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_data(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate \n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate \n",
    "score_train = model.evaluate(X_train, y_train, verbose=1)\n",
    "score_valid = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "score_test = model.evaluate(X_test_NIR, y_test_NIR)\n",
    "print(score_train)\n",
    "print(score_valid)\n",
    "print(score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(X_test, verbose=1)\n",
    "# Threshold predictions\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize\n",
    "plot_sample(X_test, y_test, preds_test, preds_test_t, ix=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(y_pred, y_true):\n",
    "     # ytrue, ypred is a flatten vector\n",
    "     y_pred = y_pred.flatten()\n",
    "     y_true = y_true.flatten()\n",
    "     current = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "     # compute mean iou\n",
    "     intersection = np.diag(current)\n",
    "     ground_truth_set = current.sum(axis=1)\n",
    "     predicted_set = current.sum(axis=0)\n",
    "     union = ground_truth_set + predicted_set - intersection\n",
    "     IoU = intersection / union.astype(np.float32)\n",
    "     return np.mean(IoU)\n",
    "    \n",
    "compute_iou(preds_test_t , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_iou(preds_val_t , y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on predicted images\n",
    "score_train_pred = model.evaluate(X_train_comp_pred, y_train_comp_pred, verbose=1)\n",
    "score_valid_pred = model.evaluate(X_valid_comp_pred, y_valid_comp_pred, verbose=1)\n",
    "score_test_pred = model.evaluate(X_test_NIR_pred, y_test_NIR_pred, verbose=1)\n",
    "print(score_train_pred)\n",
    "print(score_valid_pred)\n",
    "print(score_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
